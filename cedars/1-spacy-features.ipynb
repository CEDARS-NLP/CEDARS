{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def query_to_patterns(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Expected query will be a set of keywords separated\n",
    "    by OR keyword.\n",
    "\n",
    "    Each expression separated by OR can have expressions\n",
    "    combined by AND or NOT and the keywords can also contain\n",
    "    wildcards.\n",
    "\n",
    "    Spacy Requirements:\n",
    "    tokenization: the paper states using custom tokenizer\n",
    "    ! - negation\n",
    "    Each dictionary in a list matches one token only\n",
    "    A list matches all the dictionaries inside it (and condition)\n",
    "    A list of list contains OR conditions\n",
    "    [{\"TEXT\": {\"REGEX\": \"abc*\"}}] represents one token with regex match\n",
    "    [{\"LOWER\": \"dvt\"}] matches case-insenstitive  DVT\n",
    "    [{\"LEMMA\": \"embolus\"}] matches the lemmatized version of embolus as well in text\n",
    "\n",
    "    Implementation:\n",
    "    1. Split the query by OR\n",
    "    2. Split each expression by AND\n",
    "    3. Split each expression by NOT\n",
    "    4. Split each expression by wildcard\n",
    "    5. Convert each expression to a spacy pattern\n",
    "    6. Combine the patterns\n",
    "    7. Return the combined pattern\n",
    "    \"\"\"\n",
    "    def get_regex_dict(token):\n",
    "        return {\"TEXT\": {\"REGEX\": token}}\n",
    "\n",
    "    def get_lemma_dict(token):\n",
    "        return {\"LEMMA\": token}\n",
    "    \n",
    "    def get_negated_dict(token):\n",
    "        return {\"LOWER\": token, \"OP\": \"!\"}\n",
    "    \n",
    "    match_pattern = r'^\\s*((\\(\\s*[a-zA-Z0-9*?!]+(\\s*AND\\s*[a-zA-Z0-9*?!]+)*\\s*\\))|[a-zA-Z0-9*?!]+)(\\s*OR\\s*((\\(\\s*[a-zA-Z0-9*?!]+(\\s*AND\\s*[a-zA-Z0-9*?!]+)*\\s*\\))|[a-zA-Z0-9*?!]+))*\\s*$'\n",
    "    assert re.match(match_pattern, query), \"Query must be a set of keywords separated by OR keyword\"\n",
    "\n",
    "    or_expressions = query.split(\" OR \")\n",
    "    res = [[] for _ in range(len(or_expressions))]\n",
    "    for i, expression in enumerate(or_expressions):\n",
    "        spacy_pattern = []\n",
    "        expression = expression.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        and_expressions = expression.split(\" AND \")\n",
    "        for tok in and_expressions:\n",
    "            if \"*\" in tok or \"?\" in tok:\n",
    "                spacy_pattern.append(get_regex_dict(tok))\n",
    "            elif \"!\" in tok:\n",
    "                spacy_pattern.append(get_negated_dict(tok.replace(\"!\", \"\")))\n",
    "            else:\n",
    "                spacy_pattern.append(get_lemma_dict(tok)) \n",
    "        print(f\"{expression} -> {[spacy_pattern]}\")\n",
    "        res[i] = [spacy_pattern]\n",
    "    return  res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [ \"\"\"\n",
    "Mr First is a 60 YO M with a history of metastatic colon CA, diagnosed in 1-2008, initially with stage III disease and s/p hemicolectomy followed by adjuvant chemo, with recurrence in the liver 6 months ago, evaluated today for management of pulmonary embolism. Pt reports no prior hx of VTE, CAD or CVA/TIA. Lately he has been on regular FOLFOX chemotherapy with satisfactory results. Two weeks ago he sustained sudden onset of chest pain and dyspnea, along with palpitations. He presented to his local ER; CT angio was performed and revealed PE's in the R LL and R ML. Pt was hemodynamically stable. He was started on rivaroxaban 15 mg BID and observed overnight. B LE Dopplers did not reveal and DVT. Since discharge he has done well, with resolution of chest pain and substantial improvement of dyspnea. He is very active and denies any decrease in balance or falls. He does not take ASA but he uses ibuprophen every once in a while for joint pains. He does not drink alcohol. He has had not rectorrhagia, melena or hematuria. He tends to have mild epistaxis in the winter when the air is dry, but this has not been a problem lately.\n",
    "\"\"\", \"\"\"\n",
    "78 YO M referred for new diagnosis of CLL. Mr Down has followed-up for the same PCP for the last 15 years now, and in the last 5 y was noted to have progressive lymphocytosis, last time measure at 19,000. His PCP me and at my request sent for flow cytometry of peripheral blood, which showed a population of abnormal cells co-expressing CD5 and CD19, c/w CLL. Cell counts have otherwise been normal, with no anemia, neutropenia or thrombocytopenia. Pt feels generally well. His PCP already told him of the diagnosis. Mr Down denies fevers, NS or LAN. Wt has been stable. He denies significant fatigue and is very active. As a matter of fact, he just came back from a camping trip in the Poconos.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'LEMMA': 'dyspnea'}]]]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyspnea -> [[{'LEMMA': 'dyspnea'}]]\n",
      "[[{'LEMMA': 'dyspnea'}]]\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "7005283834202530599 DVT_0 11 12 dyspnea\n",
      "Found somehting \n",
      "\n",
      "+++++++++++++++\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "7005283834202530599 DVT_0 16 17 dyspnea\n",
      "Found somehting \n",
      "\n",
      "+++++++++++++++\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "query = \"dyspnea\"\n",
    "spacy_patterns = query_to_patterns(query)\n",
    "assert len(matcher) == 0  \n",
    "# pattern = [[{\"LOWER\": \"deep\"}, {\"LOWER\": \"vein\"}, {\"TEXT\": {\"REGEX\": \"thromb*\"}}]]\n",
    "# matcher.add(\"expandedDVT\", pattern)\n",
    "# matcher.add(\"DVT\", [[{\"LOWER\": \"dvt\"}]])\n",
    "# matcher.add(\"emboli\", [[{\"LOWER\": \"pulmonary\"}, {\"LEMMA\": \"embolus\"}]])\n",
    "for i, item in enumerate(spacy_patterns):\n",
    "    print(item)\n",
    "    matcher.add(f\"DVT_{i}\", item)\n",
    "doc = nlp(documents[0])\n",
    "for sent in doc.sents:\n",
    "    matches = matcher(sent)\n",
    "    # print(\"********* Sentence\", sent.text)\n",
    "    print(len(matches))\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = sent[start:end]                    # The matched span\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "        print(\"Found somehting \\n\\n+++++++++++++++\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15963895243908845058 DVT_1 2 5 has vein thrombus\n",
      "7005283834202530599 DVT_0 15 16 dvt\n",
      "13936575303155538993 DVT_2 19 20 clot\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]                    # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_query_to_pattern.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_query_to_pattern.py\n",
    "import pytest\n",
    "from app import nlpprocessor\n",
    "@pytest.mark.parametrize(\"query, expected\", [\n",
    "    (\"(DVT) OR (!deep AND vein AND thromb*) OR (clot) \", [3, {\"LEMMA\": \"DVT\"}, {\"LOWER\": \"deep\", \"OP\": \"!\"}]),\n",
    "    (\"bleed\", [1, {\"LEMMA\": \"bleed\"}])\n",
    "    ])\n",
    "def test_query_to_pattern(query, expected):\n",
    "    res = query_to_patterns(query)\n",
    "    assert len(res) == expected[0]\n",
    "    assert res[0][0] == expected[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.0, pytest-7.4.0, pluggy-1.2.0\n",
      "rootdir: /Users/rsingh/Programming/cedars/cedars\n",
      "configfile: pyproject.toml\n",
      "plugins: Faker-19.3.0, anyio-3.7.1\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_query_to_pattern.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_query_to_pattern[(DVT) OR (!deep AND vein AND thromb*) OR (clot) -expected0] _\u001b[0m\n",
      "\n",
      "query = '(DVT) OR (!deep AND vein AND thromb*) OR (clot) '\n",
      "expected = [3, {'LEMMA': 'DVT'}, {'LOWER': 'deep', 'OP': '!'}]\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mquery, expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33m(DVT) OR (!deep AND vein AND thromb*) OR (clot) \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m3\u001b[39;49;00m, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLEMMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mDVT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLOWER\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mdeep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mOP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}]),\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33mbleed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m1\u001b[39;49;00m, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLEMMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mbleed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}])\u001b[90m\u001b[39;49;00m\n",
      "        ])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_query_to_pattern\u001b[39;49;00m(query, expected):\u001b[90m\u001b[39;49;00m\n",
      "        res = query_to_patterns(query)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(res) == expected[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m res[\u001b[94m0\u001b[39;49;00m][\u001b[94m0\u001b[39;49;00m] == expected[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'LEMMA': 'DV...LOWER': 'DVT'} == {'LEMMA': 'DVT'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 1 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains 1 more item:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'LOWER': 'DVT'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_query_to_pattern.py\u001b[0m:63: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "Processing expression:  (DVT)\n",
      "Processing expression:  (!deep AND vein AND thromb*)\n",
      "Processing expression:  (clot) \n",
      "\u001b[31m\u001b[1m____________________ test_query_to_pattern[bleed-expected1] ____________________\u001b[0m\n",
      "\n",
      "query = 'bleed', expected = [1, {'LEMMA': 'bleed'}]\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mquery, expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33m(DVT) OR (!deep AND vein AND thromb*) OR (clot) \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m3\u001b[39;49;00m, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLEMMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mDVT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLOWER\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mdeep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mOP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}]),\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m\"\u001b[39;49;00m\u001b[33mbleed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m1\u001b[39;49;00m, {\u001b[33m\"\u001b[39;49;00m\u001b[33mLEMMA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mbleed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}])\u001b[90m\u001b[39;49;00m\n",
      "        ])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_query_to_pattern\u001b[39;49;00m(query, expected):\u001b[90m\u001b[39;49;00m\n",
      "        res = query_to_patterns(query)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(res) == expected[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m res[\u001b[94m0\u001b[39;49;00m][\u001b[94m0\u001b[39;49;00m] == expected[\u001b[94m1\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert {'LEMMA': 'bl...WER': 'bleed'} == {'LEMMA': 'bleed'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Omitting 1 identical items, use -vv to show\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Left contains 1 more item:\u001b[0m\n",
      "\u001b[1m\u001b[31mE         {'LOWER': 'bleed'}\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_query_to_pattern.py\u001b[0m:63: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "Processing expression:  bleed\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../Library/Caches/pypoetry/virtualenvs/pycedars-YmqvWyCK-py3.9/lib/python3.9/site-packages/spacy/cli/info.py:3\n",
      "  /Users/rsingh/Library/Caches/pypoetry/virtualenvs/pycedars-YmqvWyCK-py3.9/lib/python3.9/site-packages/spacy/cli/info.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_query_to_pattern.py::\u001b[1mtest_query_to_pattern[(DVT) OR (!deep AND vein AND thromb*) OR (clot) -expected0]\u001b[0m - AssertionError: assert {'LEMMA': 'DV...LOWER': 'DVT'} == {'LEMMA': 'DVT'}\n",
      "\u001b[31mFAILED\u001b[0m test_query_to_pattern.py::\u001b[1mtest_query_to_pattern[bleed-expected1]\u001b[0m - AssertionError: assert {'LEMMA': 'bl...WER': 'bleed'} == {'LEMMA': 'bleed'}\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 2.19s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_query_to_pattern.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing expression:  (DVT)\n",
      "Processing expression:  (!deep AND vein AND thromb*)\n",
      "Processing expression:  (clot) \n",
      "[{'LEMMA': 'DVT'}]\n",
      "[{'LOWER': 'deep', 'OP': '!'}, {'LEMMA': 'vein'}, {'TEXT': {'REGEX': 'thromb*'}}]\n",
      "[{'LEMMA': 'clot'}]\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "assert len(matcher) == 0  \n",
    "# pattern = [[{\"LOWER\": \"deep\"}, {\"LOWER\": \"vein\"}, {\"TEXT\": {\"REGEX\": \"thromb*\"}}]]\n",
    "# matcher.add(\"expandedDVT\", pattern)\n",
    "# matcher.add(\"DVT\", [[{\"LOWER\": \"dvt\"}]])\n",
    "# matcher.add(\"emboli\", [[{\"LOWER\": \"pulmonary\"}, {\"LEMMA\": \"embolus\"}]])\n",
    "for i, item in enumerate(query_to_patterns(query)):\n",
    "    # if len(item) == 1:\n",
    "    print(item)\n",
    "    matcher.add(f\"DVT_{i}\", [item])\n",
    "    # else:\n",
    "        # matcher.add(f\"DVT_{i}\", item)\n",
    "# matcher.add(\"DVT\", query_to_patterns(query))\n",
    "doc = nlp(\"The patient has vein thrombus in the left leg. Also had past history of DVT and pulmonary emboli clot\")\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15963895243908845058 DVT_1 2 5 has vein thrombus\n",
      "13936575303155538993 DVT_2 19 20 clot\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]                    # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.matcher.matcher.Matcher at 0x1776e09c0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_0 quick\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_0 quick\n",
      "AND_0 fox\n",
      "AND_0 jumps\n",
      "AND_0 dog\n"
     ]
    }
   ],
   "source": [
    "def create_matcher(query):\n",
    "    # Split the query by logical operators\n",
    "    tokens = query.split()\n",
    "    patterns = []\n",
    "    pattern = []\n",
    "\n",
    "    # Process tokens\n",
    "    for token in tokens:\n",
    "        if token in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            if pattern:\n",
    "                patterns.append((\"AND\", pattern))\n",
    "            patterns.append((token, []))\n",
    "            pattern = []\n",
    "        else:\n",
    "            pattern.append(token)\n",
    "    \n",
    "    if pattern:\n",
    "        patterns.append((\"AND\", pattern))\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "    for operator, pattern in patterns:\n",
    "        if operator == \"AND\":\n",
    "            for i, word in enumerate(pattern):\n",
    "                if \"*\" in word or \"?\" in word:\n",
    "                    word = word.replace(\"*\", \".*\").replace(\"?\", \".\")\n",
    "                    matcher.add(f\"AND_{i}\", [[{\"LOWER\": {\"REGEX\": word}}]])\n",
    "                else:\n",
    "                    lemma = nlp(word)[0].lemma_\n",
    "                    matcher.add(f\"AND_{i}\", [[{\"LEMMA\": lemma}]])\n",
    "        elif operator == \"OR\":\n",
    "            or_patterns = []\n",
    "            for word in pattern:\n",
    "                if \"*\" in word or \"?\" in word:\n",
    "                    word = word.replace(\"*\", \".*\").replace(\"?\", \".\")\n",
    "                    or_patterns.append([{\"LOWER\": {\"REGEX\": word}}])\n",
    "                else:\n",
    "                    lemma = nlp(word)[0].lemma_\n",
    "                    or_patterns.append([{\"LEMMA\": lemma}])\n",
    "            matcher.add(\"OR\", or_patterns)\n",
    "        elif operator == \"NOT\":\n",
    "            for i, word in enumerate(pattern):\n",
    "                if \"*\" in word or \"?\" in word:\n",
    "                    word = word.replace(\"*\", \".*\").replace(\"?\", \".\")\n",
    "                    matcher.add(f\"NOT_{i}\", [[{\"LOWER\": {\"REGEX\": word}}]], on_match=negate_match)\n",
    "                else:\n",
    "                    lemma = nlp(word)[0].lemma_\n",
    "                    matcher.add(f\"NOT_{i}\", [[{\"LEMMA\": lemma}]], on_match=negate_match)\n",
    "\n",
    "    return matcher, phrase_matcher\n",
    "\n",
    "def negate_match(matcher, doc, i, matches):\n",
    "    matches[:] = [m for m in matches if m[0] != matcher.vocab.strings[f\"NOT_{i}\"]]\n",
    "\n",
    "def process_query(doc, query):\n",
    "    matcher, phrase_matcher = create_matcher(query)\n",
    "    matches = matcher(doc)\n",
    "    return matches\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "query = \"quick AND jumps OR dog NOT fox* OR \"\n",
    "matches = process_query(doc, query)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(nlp.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'LEMMA': 'acknowledge'}],\n",
       " [{'TEXT': {'REGEX': 'severe'}}, {'LEMMA': 'bleeding'}]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\" in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E159] Can't find table 'lemma_rules' in lookups. Available tables: []\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookups\u001b[39m.\u001b[39;49mget_table(\u001b[39m\"\u001b[39;49m\u001b[39mlemma_rules\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pycedars-YmqvWyCK-py3.9/lib/python3.9/site-packages/spacy/lookups.py:232\u001b[0m, in \u001b[0;36mLookups.get_table\u001b[0;34m(self, name, default)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables:\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39m==\u001b[39m UNSET:\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE159\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, tables\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtables))\n\u001b[1;32m    233\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tables[name]\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E159] Can't find table 'lemma_rules' in lookups. Available tables: []\""
     ]
    }
   ],
   "source": [
    "nlp.vocab.has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycedars-YmqvWyCK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
